\section{Page 34}
\subsection{Exercise 2}
Assume having a square matrix $A$, which we want to maximize. $\max  A$. We assume that $A$ is a symmetric matrix.
If $A$ is non-zero, we can extract eigenvectors out of it: $ A x = \lambda x$.
We get:
\begin{gather*}
A x = \lambda x \\
x^T A x = x^T (A x) = x^T (\lambda x) = \lambda x^T x = \lambda \sum\limits_i^n | x_i | ^2
\end{gather*}
To maximize the equation given, we need to maximize $|\lambda|$, since the summation in the second term adds up to one.
So one can see that by maximizing $\lambda$, we maximize $A$.


\section{Page 37}
\subsection{Exercise 5}
A is called idempotent if $A^2 = A$. Show that each eigenvalue of an idempotent matrix is either 0 or 1.
Using this property, we can show:
\begin{gather*}
\begin{array}{cc}
\lambda x =& A x \\
=&  A^2 x \\
=& A(A x) \\
=& A(\lambda x)\\
=& \lambda (A x)\\
=& \lambda (\lambda x)\\
=& \lambda^2 x
\end{array}
\end{gather*}
Since $\lambda^2 x$ equals to $A x$, we can compute the eigenvalues:
\begin{gather*}
A x - \lambda x = 0 \\
\lambda^2 x - \lambda x = 0\\
x( \lambda ^2 - \lambda ) =0 \\
\rightarrow \lambda_1 = 1 \ \lambda_2 = 0
\end{gather*}
So we can see that the eigenvalues are either zero or one, as required.
\subsection{Exercise 6}
Here we use the same procedure as in Exercise 5.
\begin{gather*}
\lambda x = A^q x\\
\lambda x = \underbrace{AA...AA}_{q} x \\
\lambda x = \underbrace{AA...AA}_{q-1} (A x) \\
\lambda x = \underbrace{AA...AA}_{q-1} (\lambda x)\\
\lambda x = \lambda \underbrace{AA...AA}_{q-2} A x\\
\vdots \\
\lambda x = \lambda^{q} x 
\end{gather*}

Now since $A^q = 0$, we can solve the equation $A^q x = \lambda^q x$, for any $x \neq 0$.
\begin{equation*}
A^q x = \lambda^q x \rightarrow \lambda = 0
\end{equation*}
We have shown that all the eigenvalues in a positive nilpotent matrix are zero.

An example for a nilpotent matrix is the following:
\begin{equation*}
\left( \begin{array}{ccccc}
0 & 1 & & & \\
& 0 & 1 & & \\
& & \ddots &\ddots & \\
&  & & 0 & 1 \\
& & & & 0 
\end{array}\right)
\end{equation*}

\section{Page 43}
\subsection{Exercise 4}
Given that $A \in M_n$ and $A_i = \text{adj} (A)$, we want to show that equation \ref{eq:chpol} holds.
\begin{equation}
\label{eq:chpol}
\dfrac{d}{dt} p_A(t) = \sum\limits_i^n p_{A_{t}}(t)
\end{equation}
Assuming our characteristic polynomial $B =( tI - A )$ and $b_{ii} = (t-a_ii)$, we get the following equations:
\begin{gather*}
\begin{array}{cc}
\det (B) =& \sum\limits_i^n (-1)^{i+i} b_{ii} A_i\\
=& \sum\limits_i^n b_{ii} A_{i} \\
\end{array}\\
\text{taking derivative }\
\dfrac{d}{dt} \det (B) = \dfrac{d}{dt} \sum\limits_i^n (t-a_{ii}) A_{i}\\
\dfrac{d}{dt} \det (B) = \sum\limits_i^n A_{i} = \dfrac{d}{dt} p_{A}(t) \\
p_{A_{i}}(t) = \sum\limits (-1)^{2i}  a_{ii} A^{'}_i = A_i
\end{gather*}
This shows that the determinant of $B$, which is the characteristic polynomial of $A$, is $ \sum\limits_i^n p_{A_{i}}(t)$.
\subsection{Exercise 6}
We want to proof that $\text{rank} (A - \lambda I ) = n-1$.
The root of the characteristic polynomial is 0.
We can see that $\dfrac{d}{dt} p_{A}(t)$ at $t = \lambda$ is non-zero, since it only evolves calculating the principal sub matrix of $A$. From there on we can follow:
\begin{gather*}
\dfrac{d}{dt} p_{A}(\lambda) = \sum\limits_i^n A_{i} \neq 0\\
\rightarrow \sum\limits_{i=1}^n p_{A_{i}}(\lambda) \neq 0 \\
\sum\limits_{i=1}^n p_{A_{i}}(\lambda) \neq 0 \\
\exists A_i \neq 0 \rightarrow \text{rank}(A_i) = n-1
\end{gather*}
The submatrix rank follows from the column/row rank independence theorem. If one removes a row and a column from an n rank matrix, the submatrix needs to have n-1 rank, because the columns and rows are independent.

From here on we can follow, that $\text{rank} (A- \lambda I) = n-1$, since $\dfrac{d}{dt} p_{A}(\lambda) \neq 0$.
The converse is of course not true, as seen in example 1.2.7b. Consider the matrix:
\begin{equation}
\left( \begin{array}{ccccc}
1&1 & & &  \\
& 1&1 & & \\
& & \ddots& \ddots & \\
& &  & 1 & 1\\
\end{array} \right)
\end{equation}
As it can be seen, its characteristic polynomial is $1$, yet it's rank is still $n$ and not $n-1$.
\section{Page 54}
\subsection{Exercise 5}
If $A \in M_n$ and has distinct eigenvalues, show that if $AB = BA$, where $B \in M_n$, $B$ is a polynomial of degree at most $n-1$
Since $A^0 v,A^1 v,\ldots,A^{n-1} v$ are linearly independent, they form a basis for $\mathcal{R}^n$. Thus:
\begin{equation*}
Bv=c_0 A^0 v+c_1 A^1 v+\ldots+c_{n-1} A^{n-1} v=p(A)v
\end{equation*}
where 
\begin{equation*}
p(x)=c_0+c_1 x +\ldots+c_{n-1}x^{n-1}
\end{equation*}
for $r = 0,1,\ldots ,n-1$.
For some constants $c_0,c_1,\ldots,c_{n−1}$. Since B commutes with A, it commutes with all powers of A, so $B(A^r v)=A^r Bv=A^r p(A)v= p(A)(A^r v)$.

But again the vectors $A^rv$ are a basis, so $B=p(A)$, which shows that the characteristic polynomial in $B$ has $n-1$ degrees. 
%.
%%Since all matrices that commute with A are diagonalized by a basis of eigenvectors for A, and conversely, any matrix diagonalized by a basis of eigenvectors for A commutes with A, the set of matrices commuting with A forms a vector subspace of Mn of dimension n. The matrices I,A,A2,…,An−1 lie in this subspace. You can show that these matrices are linearly independent using the facts that A has n distinct eigenvalues, and that a nonzero polynomial of degree at most n−1 over a field can have at most n−1 zeros.
%\begin{gather*}
%A = \begin{pmatrix}
%  \lambda_1 I_{k_1} & 0 & \cdots & 0 \\
%  0 & \lambda_2 I_{k_2} & \cdots & 0 \\
%  \vdots  & \vdots  & \ddots & \vdots  \\
%  0 & 0 & \cdots & \lambda_m I_{k_n}
% \end{pmatrix}
% ,B=
% \begin{pmatrix}
%  B_{11} & B_{12} & \cdots & B_{1n} \\
%  B_{21} & B_{22} & \cdots & B_{2n} \\
%  \vdots  & \vdots  & \ddots & \vdots  \\
%  B_{n1} & B_{n2} & \cdots & B_{nn}
% \end{pmatrix} \\
%AB=
% \begin{pmatrix}
% \lambda_1 B_{11} & \lambda_1 B_{12} & \cdots & \lambda_1 B_{1n} \\
%  \lambda_2 B_{21} & \lambda_2 B_{22} & \cdots & \lambda_2 B_{2n} \\
%  \vdots  & \vdots  & \ddots & \vdots  \\
%  \lambda_n B_{n1} & \lambda_n B_{n2} & \cdots & \lambda_n B_{nn}
% \end{pmatrix},
% BA=
% \begin{pmatrix}
% \lambda_1 B_{11} & \lambda_2 B_{12} & \cdots & \lambda_n B_{1n} \\
%  \lambda_1 B_{21} & \lambda_2 B_{22} & \cdots & \lambda_n B_{2n} \\
%  \vdots  & \vdots  & \ddots & \vdots  \\
%  \lambda_1 B_{n1} & \lambda_2 B_{n2} & \cdots & \lambda_n B_{nn}
% \end{pmatrix}
% \end{gather*}
% From the equation given in this example , we get: 
% \begin{equation*}
%\lambda_i B_{ij} = B_{ij} \lambda_j 
% \end{equation*}
% Since all $n$ eigenvalues of $A$ are distinct, we know that the resulting equation $(\lambda_i -\lambda_j) B_{ij} = 0$ will tell us that the non-diagonal elements are zero.
% \begin{equation*}
% AB_{ij} = \begin{cases}
% 0 & \mbox{if } i \neq j \\
% B_{ij} = B_{ii} & \mbox{else } 
% \end{cases} 
% \end{equation*}
% From here on we conclude, that both $A$ and $B$ are diagonal. This means that the diagonalizable matrix $T$ will be diagonal as well, in the form of:
% \begin{equation*}
% \left( \begin{array}{cccc}
% T_{1} & & & \\
% & T_2 & & \\
% & & \ddots & \\
% & & & T_k \\
% \end{array} \right)
% \end{equation*}
% We can now write out the resulting equation of $T^{-1}AT$ and $T^{-1}BT$ respectively.
% \begin{gather*}
% T^{-1}_p \lambda_p I T_p = \lambda_p I
% \end{gather*}
 
\subsection{Exercise 6}
If $A$ is diagonalizable, which means that $A^{'} = P^{-1}AP$, so that $A'^{'}$ is diagonal. 
Since $A^{'}$ is similar to $A$, both share similar eigenvalues. That said, the characteristic polynomial of $A^{'}$ will have at least one eigenvalue, except $A$ would be the zero matrix.
We calculate:
\begin{gather*}
A^{'} = \left( \begin{array}{cccc}
a_{11}& & &  \\
& a_{22}& &  \\
& & \ddots&  \\
& & & a_{nn}
\end{array} \right) \\
p_{A}(A) = \det (A - A) =
\left( \begin{array}{cccc}
a_{11}- a_{11}& & &  \\
& a_{22} - a_{22}& &  \\
& & \ddots&  \\
& & & a_{nn} - a_{nn}
\end{array} \right) =
0
\end{gather*}
We have shown that if a matrix $A$ is diagonalizable, the characteristic polynomial with respect to itself is $0$.
\subsection{Exercise 7}
We show that every diagonalizable matrix has a square root. We assume that a matrix $A$ can be decomposed into its diagonal form $D$ by using a matrix $Q \times Q =D$, which is again a square root.
\begin{gather*}
A A = B \\
A = P^{-1} D P \\
Q Q = D\\
(P^{-1} Q P)(P^{-1} Q P) = P^{-1} Q (PP^{-1}) Q P = P^{-1} QQ P = P^{-1} D P = A
\end{gather*}
We have shown that every Matrix in $M_n$ which is diagonalizable has a square root.
\subsection{Exercise 12}
Suppose having two matrices $\Lambda = \text{diag} (\lambda_1 , \lambda_2 , \ldots ,\lambda_n)$ and $B$. We want to show that these two only commute if $B$ is diagonal itself. 
We assume that $B$ is an arbitrary matrix, while $\Lambda$ is a diagonal one.
\begin{gather*}
\Lambda = \begin{pmatrix}
  \lambda_1 I_{k_1} & 0 & \cdots & 0 \\
  0 & \lambda_2 I_{k_2} & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  0 & 0 & \cdots & \lambda_m I_{k_n}
 \end{pmatrix}
 ,B=
 \begin{pmatrix}
  B_{11} & B_{12} & \cdots & B_{1n} \\
  B_{21} & B_{22} & \cdots & B_{2n} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  B_{n1} & B_{n2} & \cdots & B_{nn}
 \end{pmatrix} \\
\Lambda B=
 \begin{pmatrix}
 \lambda_1 B_{11} & \lambda_1 B_{12} & \cdots & \lambda_1 B_{1n} \\
  \lambda_2 B_{21} & \lambda_2 B_{22} & \cdots & \lambda_2 B_{2n} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \lambda_n B_{n1} & \lambda_n B_{n2} & \cdots & \lambda_n B_{nn}
 \end{pmatrix},
 B \Lambda=
 \begin{pmatrix}
 \lambda_1 B_{11} & \lambda_2 B_{12} & \cdots & \lambda_n B_{1n} \\
  \lambda_1 B_{21} & \lambda_2 B_{22} & \cdots & \lambda_n B_{2n} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \lambda_1 B_{n1} & \lambda_2 B_{n2} & \cdots & \lambda_n B_{nn}
 \end{pmatrix}
 \end{gather*}
 From the equation given in this example , we get: 
 \begin{equation*}
\lambda_i B_{ij} = B_{ij} \lambda_j 
 \end{equation*}
 Since all $n$ eigenvalues of $A$ are distinct, we know that the resulting equation $(\lambda_i -\lambda_j) B_{ij} = 0$ will tell us that the non-diagonal elements are zero.
 \begin{equation*}
 \Lambda B_{ij} = \begin{cases}
 0 & \mbox{if } i \neq j \\
 B_{ij} = B_{ii} & \mbox{else } 
 \end{cases} 
 \end{equation*}
 From here on we conclude, that both $\Lambda$ and $B$ are diagonal. This means that the commutation can only be done if $B$ is diagonal with arbitrary entries within the diagonal.

\section{Page 61}
\subsection{Exercise 1}
Show that $A \in M_n$ has rank 1 if and only if there exist two non-zero vectors $x,y \in \mathcal{C}^n$ such that $A = xy*$
\paragraph{a}
Let $A = \left[ \begin{array}{c}
 a_1 \\ 
 \vdots \\
 a_n
\end{array} \right]$, where $a_i$ represents a row in $A$.
The rank can only be one, if 
\begin{equation*}
a_{i1} =  \alpha_{i-1} a_{11},a_{i2} = \alpha_{i-1} a_{11}, \ldots, a_{in} = \alpha_{i-1} a_{1n}
\end{equation*}
The vectors which would span $A$, can be defined as:
\begin{gather*}
x = \left( \begin{array}{c}
1 \\
\alpha_1 \\
\vdots \\
\alpha_{n-1} \\
\end{array} \right)
y* = \left( a_{11}, \ldots , a_{1n} \right)
\end{gather*}
From the definition of $x,y$ we can now use the relation $A = xy^*$ to show that these vectors are both nonzero. Otherwise $A$ would be the zero matrix, contradicting with $A$ having rank 1.

The characteristic polynomial of $A$ can be written as the sum of all eigenvalues times the .
\begin{equation*}
p_{A}(t) = t^n - (\sum\limits_{i=1}^n \lambda_i) t^{n-1} + \ldots + \det(A)
\end{equation*}
Since $\det (A) = 0 $(rank 1) then $t = 0$ is a root of $p_A$. This means that 0 is an eigenvalue of $A$. 
Suppose that $A$ has at least two non-zero distinct eigenvalues, which are defined as:
\begin{gather*}
A x_1 = \lambda_1 x_1,x_1 \neq 0
A x_2 = \lambda_2 x_2,x_2 \neq 0
\end{gather*}
Assuming $x_1$ and $x_2$ are linearly independent vectors and $A = xy^*$, we get the following relations:
\begin{gather*}
xy^* x_1 = \lambda_1 x_1 \\
x = \dfrac{\lambda_1}{y^* x_1} x_1 \\
xy^* x_2 = \lambda_2 x_2 \\
x = \dfrac{\lambda_2}{y^* x_2} x_2 \\
\end{gather*}
The quantities $y^* x_1 , y^* x_2$ are scalars so this contradicts the assumption that $x_1,x_2$ are linearly independent. If this eigenvalue $x$ exists, it has algebraic multiplicity 1. It is known that if an eigenvalue has multiplicity $k \geq 1$ then the rank of the matrix $A-\lambda I$ is $n-k$. The first already known eigenvalue (0) has at least multiplicity of $n-1$ (since we have rank 1), therefore we would have at least $n-(n-1) = 1$ eigenvalues. 

\paragraph{b}
\label{par:b}
Here we proof that the eigenvalue can be rewritten as $y^* x$.
\begin{gather*}
A v = \lambda v \\
xy^* v = \lambda v \\
y^* xy^* v = y^* \lambda v \\
y^* x y^* v = \lambda y^* v \\
(y^* x - \lambda ) y^* v = 0 \\
\rightarrow y^* x = \lambda
\end{gather*}
As required.
\paragraph{c}
The left eigenvector, using \ref{par:b}:
\begin{equation}
y^* A = (y^*x) y^*
\end{equation}

The right eigenvector is:
\begin{equation}
Ax = (xy^*) x
\end{equation}

The geometric multiplicity of the eigenvalue 0 is at least $n-1$.